{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de72daa5-b0b0-40b1-aeb3-a350d6b2708c",
   "metadata": {},
   "source": [
    "# ___Gradient Descent___\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45fafbbc-a6bc-4dce-afc5-22c40e1b4a0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T22:09:41.037366Z",
     "iopub.status.busy": "2024-05-04T22:09:41.037366Z",
     "iopub.status.idle": "2024-05-04T22:09:41.057668Z",
     "shell.execute_reply": "2024-05-04T22:09:41.056282Z",
     "shell.execute_reply.started": "2024-05-04T22:09:41.037366Z"
    }
   },
   "outputs": [],
   "source": [
    "# gradient descent is an algorithm that figures out the best fits for weights and bias, given a training dataset\n",
    "# with the goal of minimizing the cost function\n",
    "# THE PHRASE \"COST FUNCTION\" JUST MEANS THE VALUE RETURNED BY THE COST FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc7c661e-ad0b-429a-9055-6ce67dbe4309",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T22:09:41.273313Z",
     "iopub.status.busy": "2024-05-04T22:09:41.273313Z",
     "iopub.status.idle": "2024-05-04T22:09:41.282475Z",
     "shell.execute_reply": "2024-05-04T22:09:41.281466Z",
     "shell.execute_reply.started": "2024-05-04T22:09:41.273313Z"
    }
   },
   "outputs": [],
   "source": [
    "# the use of gradient descent isn't limited to linear regression, it is used almost everywhere\n",
    "# even in deep learning neural networks.\n",
    "# gradint descent is also usable with multivariate linear regression models. (models with more than two parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "742393f9-caf1-4e7f-af16-1b3165a2d494",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T22:09:41.556422Z",
     "iopub.status.busy": "2024-05-04T22:09:41.556422Z",
     "iopub.status.idle": "2024-05-04T22:09:41.575796Z",
     "shell.execute_reply": "2024-05-04T22:09:41.575796Z",
     "shell.execute_reply.started": "2024-05-04T22:09:41.556422Z"
    }
   },
   "outputs": [],
   "source": [
    "# the ultimate goal of gradient descent is to minimize the cost function j(w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f21619-0613-40b8-8656-4bc3365ffe3c",
   "metadata": {},
   "source": [
    "# $j(w, b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4660f950-c8aa-4b84-9271-a9a41dfdb66c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T22:09:42.250782Z",
     "iopub.status.busy": "2024-05-04T22:09:42.250782Z",
     "iopub.status.idle": "2024-05-04T22:09:42.264765Z",
     "shell.execute_reply": "2024-05-04T22:09:42.263965Z",
     "shell.execute_reply.started": "2024-05-04T22:09:42.250782Z"
    }
   },
   "outputs": [],
   "source": [
    "# the intuition is to start out with some bootstrap values for w and b and iteratively update them by a constant\n",
    "# evaluating the cost function at every iteration and when the cost function is sufficiently low (lower than a predesignated threshold)\n",
    "# fixate on the w and b values that yielded that cost.\n",
    "\n",
    "# FOR SOME FUNCTIONS THERE CAN BE MORE THAN ONE MINIMUM.\n",
    "# for univariate linear regression with squared error cost functions (with a bias term), the 3D plot of w, b and cost function will always \n",
    "# be hammock or bowl shaped.\n",
    "# neural networks may give different shpes, often with multiple peaks and troughs.\n",
    "\n",
    "# the essence of gradient descent is to start at a 3D coordinate on the w, b and e plot and finding the CLOSEST LOWEST point on the z axis \n",
    "# (cost function) as qick as possible (i.e finding the path of steepest descent)\n",
    "# this is a visual analogy to the mechanics of the gradient algorithm\n",
    "# CLOSEST TROUGH IS CRITICAL HERE AS WE ARE NOT TRYING TO NAVIGATE TO THE LOWEST POINT ON THE Z AXIS\n",
    "\n",
    "# the start 3D coordinate will be decided by our bootstrap values for w and b\n",
    "# THESE BOOSTRAP VALUES HAVE DECISIVE IMPACT ON THE COORDINATES OF THE TROUGH DESTINATION THE ALGORITHM ENDS AT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f525d5b-08ba-428a-b662-16723a606ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da17f4dd-2797-4477-9478-9f0d549543d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T22:09:43.134516Z",
     "iopub.status.busy": "2024-05-04T22:09:43.134516Z",
     "iopub.status.idle": "2024-05-04T22:09:43.150503Z",
     "shell.execute_reply": "2024-05-04T22:09:43.150503Z",
     "shell.execute_reply.started": "2024-05-04T22:09:43.134516Z"
    }
   },
   "outputs": [],
   "source": [
    "# to sum up, the ultimate goal is to find the path of steepest descent, to the closest local minimum of the cost function.\n",
    "# if the surface plot has many minima, the bootstrap values will dictate which minimum you end up choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430ddef6-e3e8-4f59-8102-febd30742443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "316af990-6918-4ad0-8a33-8ee5b7b9877e",
   "metadata": {},
   "source": [
    "# ___Math___\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcf1bf26-7a02-4039-90d5-b3a8f219f26d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T22:09:44.214715Z",
     "iopub.status.busy": "2024-05-04T22:09:44.214715Z",
     "iopub.status.idle": "2024-05-04T22:09:44.223029Z",
     "shell.execute_reply": "2024-05-04T22:09:44.223029Z",
     "shell.execute_reply.started": "2024-05-04T22:09:44.214715Z"
    }
   },
   "outputs": [],
   "source": [
    "# gradient descent starts off with bootstrap values for w and b, and iteratively computes the cost function updating the coefficients \n",
    "# accordingly, in every iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ec4b7b-39ad-4ba4-a4c2-f5c7165d3d43",
   "metadata": {},
   "source": [
    "# $w = w - \\alpha\\frac{\\partial j(w, b)}{\\partial{w}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a90d22e7-3a80-4bbf-a05a-6ce29a548892",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:30:46.493606Z",
     "iopub.status.busy": "2024-03-30T10:30:46.493606Z",
     "iopub.status.idle": "2024-03-30T10:30:46.499606Z",
     "shell.execute_reply": "2024-03-30T10:30:46.499098Z",
     "shell.execute_reply.started": "2024-03-30T10:30:46.493606Z"
    }
   },
   "outputs": [],
   "source": [
    "# the above means that update w by subtracting the term on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518f9033-7bc5-4a37-9958-64c5ba68bbf7",
   "metadata": {},
   "source": [
    "# $\\alpha\\frac{\\partial j(w, b)}{\\partial{w}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a05b3762-d845-461d-bbfc-0b0230443375",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T22:16:42.190491Z",
     "iopub.status.busy": "2024-05-04T22:16:42.190491Z",
     "iopub.status.idle": "2024-05-04T22:16:42.200728Z",
     "shell.execute_reply": "2024-05-04T22:16:42.200728Z",
     "shell.execute_reply.started": "2024-05-04T22:16:42.190491Z"
    }
   },
   "outputs": [],
   "source": [
    "# alpha is the learning rate, which decides how much impact the term on its right is to have on w\n",
    "# usually alpha is a small positive real number, and it controls the size of the step you take downhill from your starting point to the local minimum.\n",
    "# with large alphas, the descent will take huge leaps, but this is not necessarily a good thing though!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128d2ef5-d5c9-4abd-9f68-f36fd6050f2a",
   "metadata": {},
   "source": [
    "# $\\frac{\\partial{j(w, b)}}{\\partial{w}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "873b69bd-0c25-4cfe-b60d-1c91ea3e4516",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:30:47.090387Z",
     "iopub.status.busy": "2024-03-30T10:30:47.090387Z",
     "iopub.status.idle": "2024-03-30T10:30:47.109126Z",
     "shell.execute_reply": "2024-03-30T10:30:47.108118Z",
     "shell.execute_reply.started": "2024-03-30T10:30:47.090387Z"
    }
   },
   "outputs": [],
   "source": [
    "# the term above specifies which direction the gradient descent needs to proceed!\n",
    "# it is the partial derivative of the cost function j(w, b) with respect to w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8842dd5-24a4-4e49-bdd5-2ff5a50b42eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:30:47.254667Z",
     "iopub.status.busy": "2024-03-30T10:30:47.254667Z",
     "iopub.status.idle": "2024-03-30T10:30:47.266425Z",
     "shell.execute_reply": "2024-03-30T10:30:47.266425Z",
     "shell.execute_reply.started": "2024-03-30T10:30:47.254667Z"
    }
   },
   "outputs": [],
   "source": [
    "# the bias term is also updated by gradient descent in a similar way, by subtracting alpha * partial derivative of the cost function j(w, b) with\n",
    "# respect to b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc64701-d956-4586-b75b-05a0871344cf",
   "metadata": {},
   "source": [
    "# $b = b - \\alpha\\frac{\\partial j(w, b)}{\\partial{b}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8a89b20-cc4d-4914-a057-89d19385d5b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:30:47.705026Z",
     "iopub.status.busy": "2024-03-30T10:30:47.705026Z",
     "iopub.status.idle": "2024-03-30T10:30:47.725702Z",
     "shell.execute_reply": "2024-03-30T10:30:47.724692Z",
     "shell.execute_reply.started": "2024-03-30T10:30:47.705026Z"
    }
   },
   "outputs": [],
   "source": [
    "# gradient descent will keep iterating until the algorithm converges, i.e where the cost function doesn't change much with each iteration\n",
    "# i.e when the predictions have become sufficiently close to the actual values :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb060db-5887-408e-afd3-7f710f229807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3dde528-4aab-44cb-958f-0f863e112397",
   "metadata": {},
   "source": [
    "## ___IMPORTANT___\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3af54f67-34af-4fac-a7a7-e3b582378d9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:30:47.988587Z",
     "iopub.status.busy": "2024-03-30T10:30:47.988587Z",
     "iopub.status.idle": "2024-03-30T10:30:48.003093Z",
     "shell.execute_reply": "2024-03-30T10:30:48.003093Z",
     "shell.execute_reply.started": "2024-03-30T10:30:47.988587Z"
    }
   },
   "outputs": [],
   "source": [
    "# FOR GRADIENT DESCENT BOTH W AND B NEEDS TO BE UPDATED SIMULTANEOUSLY.\n",
    "# BECAUSE THE PARTIAL DERIVATIVE ON THE RIGHT DEPENDS ON BOTH W AND B, DOING UPDATES SEQUENTIALLY WILL RESULT IN THE LASTLY UPDATED PARAMETER USING\n",
    "# THE UPDATED VALUE FOR THE OTHER PARAMETER INSTEAD OF THE VALUE FROM PREVIOUS ITERATION.\n",
    "\n",
    "# THE TRICK TO CIRCUMNAVIGATE THIS PROBLEM IS TO FIRST COMPUTE THE PARTIAL DERIVATE TERM FOR W AND B SEPARATELY AND THEN TO DO THE UPDATES.\n",
    "# I.E,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539c65ec-1e8d-4100-8a27-c7592bfe1655",
   "metadata": {},
   "source": [
    "# $\\Delta{w} = \\alpha\\frac{\\partial}{\\partial{w}}j(w, b)$\n",
    "# $\\Delta{b} = \\alpha\\frac{\\partial}{\\partial{b}}j(w, b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "274851ac-aa18-4fc2-951f-2ef4116d4fd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:30:48.602132Z",
     "iopub.status.busy": "2024-03-30T10:30:48.602132Z",
     "iopub.status.idle": "2024-03-30T10:30:48.619947Z",
     "shell.execute_reply": "2024-03-30T10:30:48.618881Z",
     "shell.execute_reply.started": "2024-03-30T10:30:48.602132Z"
    }
   },
   "outputs": [],
   "source": [
    "# WHEN IT IS DONE THIS WAY, THE TEMPORARIES W DASH AND B DASH HAVE BEEN COMPUTED USING THE SAME VALUES FOR W AND B \n",
    "# VALUES FROM THE CURRENT ITERATION, NOT THE UPDATED ONES\n",
    "# THEN WE PROCCED WITH THE UPDATES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ce0b1d-0b65-48f5-8896-f21431054a0b",
   "metadata": {},
   "source": [
    "# $w = w - \\Delta{w}$\n",
    "# $b = b - \\Delta{b}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14cc145d-8062-446a-a00f-90d974abb406",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:33:40.489408Z",
     "iopub.status.busy": "2024-03-30T10:33:40.487276Z",
     "iopub.status.idle": "2024-03-30T10:33:40.493721Z",
     "shell.execute_reply": "2024-03-30T10:33:40.493185Z",
     "shell.execute_reply.started": "2024-03-30T10:33:40.489408Z"
    }
   },
   "outputs": [],
   "source": [
    "# if we dit it sequentially,"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f346456a-a8fe-42c4-bce7-00ff0da53a7f",
   "metadata": {},
   "source": [
    "# $w = w - \\alpha\\frac{\\partial}{\\partial{w}}j(w, b) \\rightarrow (1)$\n",
    "# $b = b - \\alpha\\frac{\\partial}{\\partial{b}}j(w, b) \\rightarrow (2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d993c4d6-d6ac-4b60-b6e6-debc9204244f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T22:25:48.601981Z",
     "iopub.status.busy": "2024-05-04T22:25:48.601981Z",
     "iopub.status.idle": "2024-05-04T22:25:48.612021Z",
     "shell.execute_reply": "2024-05-04T22:25:48.612021Z",
     "shell.execute_reply.started": "2024-05-04T22:25:48.601981Z"
    }
   },
   "outputs": [],
   "source": [
    "# when the second update takes place, i.e the update of b (2), the value of w would no longer be the same.\n",
    "# since the cost function consumes w, b will be updated using a partial derivative computed using the updated w NOT THE ORIGINAL w!\n",
    "# DOING SEQUENTIAL UPDATES WILL WORK MORE OR LESS CORRECTLY, BUT THAT ISN'T THE WAY IT IS SUPPOSED TO BE DONE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452acf73-ef91-4b00-b134-b4bccfd309e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
